---
title: "forecasting"
format: 
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    toc-location: left
    number-sections: true
    code-fold: false
editor: visual
execute:
  echo: true
  warning: false
---

```{r}
library(dplyr) 
library(lubridate) 
library(xts) 
library(tsoutliers) 
library(parallel) 
library(doParallel) 
library(seastests)
library(tseries)
library(leaps)
library(gets)
library(lmtest)
library(stats)
library(FinTS)
```

Let's clean the data:

```{r}
df<- read.csv("Data/sacrois_complete.csv")
print(unique(df$X3A_CODE))
print(length(unique(df$X3A_CODE))) # 44 species
df$DATE_SEQ <- as.Date(df$DATE_SEQ)
str(df)

df <- df %>%
  rename("Date" = "DATE_SEQ") %>%
  rename("Species" = "X3A_CODE") %>%
  rename("LandingsKG" = "QUANT_POIDS_VIF_MOYENNE") %>%
  rename("LandingsEUR" = "MONTANT_EUROS_MOYENNE")%>%
  filter(Date >= "2001-01-01" & Date < "2024-01-01")
```

We are getting rid of the late 1999 data and beginning of 2024 data.

# Create time series for each species

We will create a `ts_list()` , a list of lists, that contains the time-series of each species of the data set. The frequency is **365** because we are dealing with daily data.

```{r}
# Get the list of unique species
species_list <- unique(df$Species)

# Define the exact date range we want
start_date <- as.Date("2000-01-01")
end_date <- as.Date("2023-12-31")

# Create a complete date sequence
date_seq <- seq(start_date, end_date, by = "day")

# Create a time series for each species
ts_list <- list()
for (species in species_list) {
  # Filter data for the current species and aggregate by date
  species_data <- df %>% 
    filter(Species == species) %>% 
    group_by(Date) %>%
    summarize(LandingsKG = sum(LandingsKG, na.rm = TRUE)) %>%
    ungroup()
  
  # Create a data frame with all dates
  full_data <- data.frame(Date = date_seq)
  
  # Merge with the species data
  full_data <- left_join(full_data, species_data, by = "Date")
  
  # Fill NA values with 0
  full_data$LandingsKG[is.na(full_data$LandingsKG)] <- 0
  
  # Create a ts object
  ts_object <- ts(full_data$LandingsKG, 
                  start = c(2000, 1), 
                  frequency = 365)
  
  # Add the ts object to the list
  ts_list[[paste0("ts_", species)]] <- ts_object
}

# Create a vector with all the TS names
ts_names <- names(ts_list)

# Print the names of the time series
print(ts_names)
```

# Seasonality Detection

We want to apply the `combined_test` on each time series to sea if there is any seasonality. You should note that:

-   **H1** means **seasonality** *(alternative hypothesis)*

-   **H0** means **NO seasonality** *(null hypothesis)*

-   if p \< 0.05 then H1 : seasonality

```{r}
seasonal_combined_test <- c()
seasonal_seasdum <- c()

# Loop over each TS
for (ts_name in names(ts_list)) {
  ts_data <- ts_list[[ts_name]]  # Retrieve data from the time series list
  
  # Apply the combined_test() test to the time series
  ct_res <- combined_test(ts_data)
  
  # Check each p-value and display results if at least one valid H1
  ct_results <- ct_res$stat
  ct_pvals <- ct_res$Pval
  if (ct_results == TRUE) {
    cat("\n")
    cat("\n")
    print(paste0("Combined_test results for the series ", ts_name))
    print(ct_res)
    seasonal_combined_test <- c(seasonal_combined_test, ts_name)
  }
  
  # # Apply the seasdum() test to the time series
  # sd_res <- seasdum(ts_data)
  # 
  # # Check p-value and display results if < 0.05
  # if (sd_res$Pval < 0.05) {
  #   cat("\n")
  #   cat("\n")
  #   print(paste0("Résultats du seasdum pour la série ", ts_name))
  #   print(sd_res)
  #   seasonal_seasdum <- c(seasonal_seasdum, ts_name)
  # }
}

assign("seasonal_combined_test", seasonal_combined_test, envir = .GlobalEnv)
```

[The `combined_test()` function combines three tests:]{.underline}

-   QS test (Quantile Spectral test)

-   QS-R test (QS test on residuals)

-   KW-R test (Kruskal-Wallis test on residuals)

[The function considers a time series as seasonal if:]{.underline}

-   The p-value of QS-R is \< 0.01 OR the p-value of KW-R is \< 0.001

-   OR if both the p-value of QS is \< 0.01 AND the p-value of KW-R is \< 0.01

[Here are the time series that have Seasonality:]{.underline}

```{r}
seasonal_combined_test
```

We need to correct that seasonality among those time series.

```{r}
for (ts_name in seasonal_combined_test) {
  ts_data <- ts_list[[ts_name]]  # Retrieve time series data from the list
  decomp <- stl(ts_data, s.window = 365)  # STL decomposition
  seasonal <- decomp$time.series[, "seasonal"]  # Get the seasonal component
  ts_data_adjusted <- ts_data - seasonal  # Correcting the seasonal component
  ts_list[[ts_name]] <- ts_data_adjusted  # Update the adjusted time series in the list
}
```

Now that this is done, let's check again for any seasonality:

```{r}
seasonal_combined_test <- c()
seasonal_seasdum <- c()

# Loop over each TS
for (ts_name in names(ts_list)) {
  ts_data <- ts_list[[ts_name]]  # Retrieve data from the time series list
  
  # Apply the combined_test() test to the time series
  ct_res <- combined_test(ts_data)
  
  # Check each p-value and display results if at least one valid H1
  ct_results <- ct_res$stat
  ct_pvals <- ct_res$Pval
  if (ct_results == TRUE) {
    cat("\n")
    cat("\n")
    print(paste0("Combined_test results for the series ", ts_name))
    print(ct_res)
    seasonal_combined_test <- c(seasonal_combined_test, ts_name)
  }
}
```

It's weird that we have such a difference between the first two tests and the last one. There are quite the opposite, in term of P-values.

For now we will focus on the first two tests *(QS & QS-R)* tests as our results for seasonality. According to them, **there is no seasonality left.**

# Stationarity

Now, let's take care of stationarity. Most of the forecasting models we are going to use need stationary time-series. We will use the `adf.test` from the [tseries](https://www.rdocumentation.org/packages/tseries/versions/0.10-54/topics/adf.test) package.

Note that for this test we have:

-   H0: série non stationnaire (possède une racine unitaire)

-   H1: série stationnaire (n'en possède pas)

-   if p\<0.05 : the serie is stationary

```{r}
#| warning: false
# First round of differencing
for (ts_name in names(ts_list)) {
  ts_data <- ts_list[[ts_name]]  # Retrieve time series data from the list
  adf_result <- adf.test(ts_data)  # Apply ADF test for stationarity
  #print(adf_result)
  
  # Check if P-Value > 0.05
  if (adf_result$p.value > 0.05) {
    # Apply a difference to the time series if it is not stationary
    ts_list[[ts_name]] <- diff(ts_data)
    print(paste("The TS", ts_name, "has been differentiated"))
  }
  else {
    print(paste("The TS", ts_name, "is stationary"))
  }
}

# Second round of differencing if necessary
for (ts_name in names(ts_list)) {
  ts_data <- ts_list[[ts_name]]  # Retrieve time series data from the list
  adf_result <- adf.test(ts_data)  # Apply ADF test for stationarity
  #print(adf_result)
  
  # Check if P-Value > 0.05
  if (adf_result$p.value > 0.05) {
    print(paste("The TS", ts_name, "is still not stationary"))
    # Apply a difference to the time series if it is not stationary
    ts_list[[ts_name]] <- diff(ts_data)
    print(paste("The TS", ts_name, "has been differentiated a second time"))
  }
}

# Final check
for (ts_name in names(ts_list)) {
  ts_data <- ts_list[[ts_name]]  # Retrieve time series data from the list
  adf_result <- adf.test(ts_data)  # Apply ADF test for stationarity
  #print(adf_result)
  
  if (adf_result$p.value > 0.05) {
    # Check if P-Value > 0.05
    cat(paste0("\033[1m\033[4m\033[31mThe time series ", ts_name, " is still not stationary:\033[0m\n"))
  }
}
```

It seems that every time-series is now stationary. Let's check if they all have the same length

```{r}
str(ts_list)
```

They do.

Now, before forecasting we are going to use a variables selection method. This will help us to reduce the number of irrelevant variables of our models.

After that, we will apply econometrics and machine learning forecasting models.

From now on, we'll reason as follows:

-   $Y_t$: will be the variable we seek to predict

-   $X_k,_t$: will be the explanatory variables, which will add information to enable the models to better predict our $Y_t$.

For your information, here's a table summarizing our variables. In bold are the dynamic variables in [Ifremer's IAM forecasting model](https://archimer.ifremer.fr/doc/00784/89579/). As these variables are already considered dynamic, they are the only ones we won't attempt to predict.

| HKE     | European hake              | MUT     | Red mullet               |
|---------|----------------------------|---------|--------------------------|
| **NEP** | **Norway lobster**         | **ARA** | **Red shrimp**           |
| **DPS** | **Deep-water rose shrimp** | ANE     | European anchovy         |
| BES     | Belone                     | BOG     | Bogue                    |
| BSS     | European seabass           | BZX     | Bonitos                  |
| CTL     | Cuttlefish, bobtail squids | DCP     | Natantian decapods       |
| DEX     | Dentex                     | DPS     | Deep-water rose shrimp   |
| ELX     | River eels                 | FIN     | Finfishes                |
| FLX     | Flatfishes                 | FOX     | Forkbeards               |
| GUX     | Gurnards, searobins        | JAX     | Jack and horse mackerels |
| JLX     | Murex shells               | JOD     | John dory                |
| MAX     | Mackerels                  | MGR     | Meagre                   |
| MNZ     | Monkfishes                 | MUL     | Mullets                  |
| MUX_bis | Red mullets                | OCT     | Octopuses, etc.          |
| PAC     | Common pandora             | PEN     | Penaeus shrimps          |
| PIL     | European pilchard          | POD     | Poor cod                 |
| SAA     | Round sardinella           | SBA     | Axillary seabream        |
| SBG     | Gilthead seabream          | SBX     | Porgies, seabreams       |
| SOX     | Soles                      | SQY     | Squillids                |
| SQZ     | Inshore squids             | SRX     | Rays, stingrays, mantas  |
| SWM     | Swimming crabs, etc.       | TUX     | Tuna-like fishes         |
| VLO     | Spiny lobsters             | WEX     | Weevers                  |
| WHB     | Blue whiting               | XOX     | Sandlances               |

: Different species of the SACROIS dataset

[Note]{.underline}: `ZZZ` therefore includes all species not mentioned in the table above. It is therefore a collection of several species.

First, let's create a single dataframe:\

```{r}
# Starting from 1st January 2001 until total number of values : 8766
dates <- seq(as.Date("2000-01-01"), by = "day", length.out = 8766)

# Empty data frame
df <- data.frame(Date = dates)

for (nom in names(ts_list)) {
  # Extract species names and delete "ts_"
  nom_espece <- sub("^ts_", "", nom)
  
  # Add to a new column
  df[[nom_espece]] <- as.vector(ts_list[[nom]])
}

write.csv(df, "Data/data_clean.csv", row.names = FALSE)
str(df)
```

For the forecasting models we are going to use 80% of the df as training and the last 20% as testing base. The idea behind this is to train the models on the training base and compare their results with the "reality" *(meaning the testing base)*. This will allow us to compute some performance indicators and compare the models.

# Cleaning Variables

But first, there is one problem. Some species have long period with no data at all, meaning only 0's. We have to get rid of the species that have either:

-   only 0's on the training base

-   only 0's on the testing base

The forecasting models can't perform with either a training or testing base full of 0's.

```{r}
df <- read.csv("Data/data_clean.csv")
```

```{r}
n <- nrow(df)
train_size <- round(0.8 * n)
print(train_size)

train <- df[1:train_size, 2:ncol(df)] # Get rid of "Date"

constant_columns <- sapply(train, function(col) length(unique(col)) == 1)
constant_column_names <- names(constant_columns)[constant_columns]
constant_column_names
```

We have to get rid of `DCP` because it's constant

```{r}
n <- nrow(df)
test_size <- round(0.2 * n)
print(test_size)

test <- df[(train_size+1):n, 2:ncol(df)] # Get rid of "Date"

constant_columns <- sapply(test, function(col) length(unique(col)) == 1)
constant_column_names <- names(constant_columns)[constant_columns]
constant_column_names

```

We have to get rid of `ANE, DEX, DPS, FIN, JLX, PEN, WEX, WHB` because it's constant

```{r}
df <- df %>%
  select(-c("ANE", "DCP", "DEX", "DPS", "FIN", "JLX", "PEN", "WHB"))
names(df)
```

So now let's re do the training and testing bases

```{r}
train <- df[1:train_size, 2:ncol(df)] # Get rid of "Date"
test <- df[(train_size+1):n, 2:ncol(df)] # Get rid of "Date"
```

# Forecasting: BES

Let's start the variable selection process with `BES` as our $Y_t$

## Variable selections

The variable selection process if composed of two methods, the `get` and the `BestSubSet`.\
We will try to take both of them into account.

### BestSubSet Method

```{r}
leaps <- regsubsets(ANE ~ .,data= train, nbest=1, method=c("exhaustive"))
leaps
res.sum <- summary(leaps) 
data.frame(Adj.R2=which.max(res.sum$adjr2),
           CP=which.min(res.sum$cp),
           BIC=which.min(res.sum$bic))

#  plot  a  table  of  models  showing  variables  in  each  model 
plot(leaps,scale="adjr2",main="Adjusted R2")
plot(leaps,scale="Cp",main="Critère de Mallow's Cp")
plot(leaps, scale = "bic", main = "BIC")

```

it seems that the following 9 variables are chosen by our 3 selection criteria: `FOX, GUX, JOD, MNZ, PAC, SQZ, SRX, ZZZ.`

### Gets Method

```{r}
mX <- data.matrix(train[, !names(train) %in% c("BES")])
y <- train$BES

# ARX Initial
modele_arx <- arx(y, mc = TRUE, ar = 1, mxreg = mX, vcov.type = "ordinary")


seuil_p_value <- 0.05
variables <- colnames(mX)
VRAI <- TRUE

while (VRAI) {
  modele_arx <- arx(y, mc = TRUE, ar = 1, mxreg = mX, vcov.type = "ordinary")
  
  p_values <- modele_arx[["mean.results"]][["p-value"]][-c(1, 2)] # Exclure la constante et AR(1)
  max_p_value <- max(p_values)
  
  if (max_p_value > seuil_p_value) {
    variable_a_supprimer <- variables[which.max(p_values)]
    variables <- setdiff(variables, variable_a_supprimer)
    mX <- mX[, variables, drop = FALSE]
  } else {
    VRAI <- FALSE
  }
}


arx_final <- arx(y, ar = 1, mxreg = mX, vcov.type = "ordinary")

modele_gets <- getsm(arx_final)
str(mX)
```

According to the results we should have 10 variables in our models: `BZX, FIN, FOX, GUX, JOD, MNZ, PAC, SBA, SQZ, SRX.`

Note that p\<0.05 for the Ljung-Box ARCH(1) which means that we have probably conditional heteroskedasticity. Most machine learning and econometric forecasting models assume that the residuals (errors) are homoscedastic, meaning they have constant variance over time. The presence of ARCH effects violates this assumption, and using the selected variables without accounting for heteroskedasticity can lead to biased and inefficient model estimates.

[Let's test that with different tests:]{.underline}

```{r}
# Fit a linear regression model
model_fit <- lm(ANE ~ ., data = train)

# Obtain residuals
residuals <- residuals(model_fit)

# Engle's LM test for ARCH effects
arch_test <- ArchTest(residuals, lags = 1)
print(arch_test)

# Breusch-Pagan test for heteroskedasticity
bp_test <- bptest(model_fit)
print(bp_test)

# Ljung-Box portmanteau test for ARCH effects
squared_residuals <- residuals^2
ljung_box_arch <- Box.test(squared_residuals, lag = 1, type = "Ljung-Box")
print(ljung_box_arch)
```

All three tests provide strong evidence against the assumption of constant variance (homoscedasticity) and suggest the presence of ARCH effects (conditional heteroskedasticity) in our data.

We'll continue our analysis and apply machine learning and econometric models, but bear in mind that the results are potentially biased for these reasons.

## ARX with Get

```{r}
set.seed(123)
n <- nrow(df)
train_size <- round(0.8 * n)
train_data <- df[1:train_size, 2:ncol(df)]
test_data <- df[(train_size+1):n, 2:ncol(df)]
y_real <- test_data$ANE


mX_train <- data.matrix(train_data[,!names(train_data) %in% c("ANE", "DCP")]) 
mX_test <- data.matrix(test_data[,!names(train_data) %in% c("ANE", "DCP")])

y_train <- train_data$ANE
y_test <- test_data$ANE

model <- arx(y_train, mc = TRUE, ar = 1, mxreg = mX_train, vcov.type = "ordinary")
n_test <- nrow(test_data)
p_arxget <- predict(model, n.ahead = n_test, newmxreg = mX_test)
p_arxget <- as.numeric(p_arxget)

# Plot
plot(y_real, type = "l", col = "black", lwd = 2, ylim = range(c(y_real)))
lines(p_arxget, col = "red", lwd = 2)

rmse <- sqrt(mean((y_real - p_arxget)^2, na.rm = TRUE))
print(paste("RMSE :", rmse)) 
```
